{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting link:  https://github.com/ros2/ros2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ros2'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding subdirectory: https://github.com/ros2/ros2/tree/rolling/README.md\n",
      "Adding subdirectory: https://github.com/ros2/ros2/tree/rolling/.gitignore\n",
      "Adding subdirectory: https://github.com/ros2/ros2/tree/rolling/CODEOWNERS\n",
      "Adding subdirectory: https://github.com/ros2/ros2/tree/rolling/ros2.repos\n",
      "Adding subdirectory: https://github.com/ros2/ros2/tree/rolling/src/.gitkeep\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('67546ba6d039caad6021408d'), ObjectId('67546ba6d039caad6021408e'), ObjectId('67546ba6d039caad6021408f'), ObjectId('67546ba6d039caad60214090'), ObjectId('67546ba6d039caad60214091')], acknowledged=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See README for more info on how the DataCollectionPipeline works\n",
    "# The ETL pipeline is part of the DataCollectionPipeline\n",
    "# Remove the time.sleep(1) line if you are sure you won't get blocked from a webpage for requesting too often\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "\n",
    "# Input into the Data Collection Pipeline is a list of links to domains\n",
    "links = ['https://www.ros.org/','https://docs.nav2.org/','https://moveit.ai/','https://gazebosim.org/home', 'https://github.com/ros2/ros2', 'https://github.com/ros-navigation/navigation2', 'https://github.com/moveit/moveit2', 'https://github.com/gazebosim/gazebo-classic']\n",
    "links = ['https://www.ros.org/', 'https://github.com/ros2/ros2']\n",
    "\n",
    "# Create a mongoDB connection\n",
    "try:\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    load_dotenv(sys.path[1] + \"/.env\")\n",
    "DATABASE_HOST = os.getenv(\"DATABASE_HOST\")\n",
    "mongoHost =  pymongo.MongoClient(DATABASE_HOST)\n",
    "mongoDatabase =  mongoHost[\"twin\"]\n",
    "\n",
    "# ETL pipeline\n",
    "# Extract data from links and their subdirectories(using crawlers)\n",
    "documents = []\n",
    "codes = []\n",
    "for link in links:\n",
    "    # Web scraper/crawler for github links\n",
    "    if \"https://github.com\" in link:\n",
    "        # Do not revisit a link already in the database\n",
    "        mongoCollection = mongoDatabase[\"Github\"]\n",
    "        result = mongoCollection.find_one({\"link\": link})\n",
    "        if result is None:\n",
    "            print(\"Visiting link: \", link)\n",
    "            # Modified GithubCrawler from LLM-Engineer for scraping github\n",
    "            local_temp = tempfile.mkdtemp()\n",
    "            try:\n",
    "                os.chdir(local_temp)\n",
    "                subprocess.run([\"git\", \"clone\", link])\n",
    "                repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])\n",
    "                tree = {}\n",
    "                for root, _, files in os.walk(repo_path):\n",
    "                    dir = root.replace(repo_path, \"\").lstrip(\"/\")\n",
    "                    if dir.startswith((\".git\", \".toml\", \".lock\", \".png\")):\n",
    "                        continue\n",
    "                    for file in files:\n",
    "                        if file.endswith((\".git\", \".toml\", \".lock\", \".png\")):\n",
    "                            continue\n",
    "                        file_path = os.path.join(dir, file)\n",
    "                        with open(\n",
    "                            os.path.join(root, file), \"r\", errors=\"ignore\"\n",
    "                        ) as f:\n",
    "                            tree[file_path] = f.read().replace(\" \", \"\")\n",
    "            except Exception:\n",
    "                print(f\"Error scrapping {link}\")\n",
    "            finally:\n",
    "                shutil.rmtree(local_temp)\n",
    "                # Correct the link\n",
    "                r = requests.get(link)\n",
    "                soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "                # Find the file path to any of the files in the repository\n",
    "                link_element = soup.find(\"a\", attrs={\"class\": \"Link--primary\"})\n",
    "                path = link_element.get(\"href\")\n",
    "                path = path.rsplit(\"/\", 1)[0]\n",
    "                # Push all the subdirectories to mongo\n",
    "                for subdirectory in tree:\n",
    "                    print(\n",
    "                        f\"Adding subdirectory: https://github.com{path}/{subdirectory}\"\n",
    "                    )\n",
    "                    text = tree[subdirectory]\n",
    "                    # Transform the data\n",
    "                    # Get rid of repeating \\n characters and spaces\n",
    "                    text = text.replace(\"\\t\", \" \")\n",
    "                    text = text.replace(\"\\n\", \" \")\n",
    "                    text_len = len(text)\n",
    "                    for i in range(text_len):\n",
    "                        while i + 1 < text_len and text[i] == \" \" and text[i + 1] == \" \":\n",
    "                            text = text[:i] + text[i + 1 :]\n",
    "                            text_len -= 1\n",
    "                    codes.append(\n",
    "                        {\n",
    "                            \"link\": \"https://github.com\"\n",
    "                            + path\n",
    "                            + \"/\"\n",
    "                            + subdirectory,\n",
    "                            \"type\": \"Github\",\n",
    "                            \"content\": text,\n",
    "                        }\n",
    "                    )\n",
    "        else:\n",
    "            print(\"Already visited: \", link)\n",
    "    # Web scraper/crawler for other links(Documents)\n",
    "    else:\n",
    "        # Do not revisit a link already in the database\n",
    "        mongoCollection = mongoDatabase[\"Document\"]\n",
    "        result = mongoCollection.find_one({\"link\": link})\n",
    "        if result is None:\n",
    "            # Get all text in the website\n",
    "            r = requests.get(link)\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "            text = soup.get_text()\n",
    "            # Transform the data\n",
    "            # Get rid of repeating \\n characters and spaces\n",
    "            text = text.replace(\"\\t\", \" \")\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            text_len = len(text)\n",
    "            for i in range(text_len):\n",
    "                while i + 1 < text_len and text[i] == \" \" and text[i + 1] == \" \":\n",
    "                    text = text[:i] + text[i + 1 :]\n",
    "                    text_len -= 1\n",
    "            documents.append({\"link\": link, \"type\": \"Document\", \"content\": text})\n",
    "            # Also crawl through all subdirectorys in the link(related links)\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            subdirectories = [a.get(\"href\") for a in soup.find_all(\"a\")]\n",
    "            for subdirectory in subdirectories:\n",
    "                if (\n",
    "                    subdirectory is not None\n",
    "                    and mongoCollection.find_one({\"link\": link + subdirectory})\n",
    "                    is not None\n",
    "                ):\n",
    "                    print(\"Adding subdirectory: \", link + subdirectory)\n",
    "                    links.append(link + subdirectory)\n",
    "        else:\n",
    "            print(\"Already visited: \", link)\n",
    "    # Avoid spamming sites\n",
    "    time.sleep(1)\n",
    "# Each document has a link, type(github or other), and content(text)\n",
    "# You can go to Tools/mongoTools to view the inserted documents\n",
    "mongoCollection = mongoDatabase[\"Document\"]\n",
    "mongoCollection.insert_many(documents)\n",
    "mongoCollection = mongoDatabase[\"Github\"]\n",
    "mongoCollection.insert_many(codes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
